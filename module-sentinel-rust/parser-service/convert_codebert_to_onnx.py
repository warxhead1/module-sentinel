#!/usr/bin/env python3
"""
Convert CodeBERT PyTorch model to ONNX format for Rust inference.
"""

import torch
import torch.onnx
from transformers import RobertaModel, RobertaConfig
import json
import os
from pathlib import Path

def convert_codebert_to_onnx():
    """Convert the downloaded CodeBERT PyTorch model to ONNX format."""
    
    print("üîÑ Converting CodeBERT PyTorch model to ONNX...")
    
    # Paths
    models_dir = Path("models")
    config_path = models_dir / "models" / "config.json"
    pytorch_model_path = models_dir / "models" / "codebert-base-pytorch.bin"
    onnx_output_path = models_dir / "code_similarity.onnx"
    
    # Check if files exist
    if not config_path.exists():
        print(f"‚ùå Config file not found: {config_path}")
        return False
        
    if not pytorch_model_path.exists():
        print(f"‚ùå PyTorch model not found: {pytorch_model_path}")
        return False
    
    try:
        # Load config
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        
        print(f"üìÑ Loaded config: {config_dict['model_type']} with {config_dict['num_hidden_layers']} layers")
        
        # Create RoBERTa config from the loaded configuration
        config = RobertaConfig.from_dict(config_dict)
        
        # Initialize model with config (don't load weights yet)
        model = RobertaModel(config)
        
        # Load the state dict from the PyTorch model file
        print(f"üîß Loading PyTorch weights from {pytorch_model_path}")
        state_dict = torch.load(pytorch_model_path, map_location='cpu')
        
        # Load weights into model
        model.load_state_dict(state_dict)
        model.eval()
        
        print(f"‚úÖ Model loaded successfully")
        print(f"   - Vocab size: {config.vocab_size}")
        print(f"   - Hidden size: {config.hidden_size}")
        print(f"   - Max position embeddings: {config.max_position_embeddings}")
        
        # Create dummy input for ONNX export
        # CodeBERT typically expects sequences up to 512 tokens
        batch_size = 1
        sequence_length = 128  # Reasonable size for code snippets
        
        dummy_input_ids = torch.randint(0, config.vocab_size, (batch_size, sequence_length))
        dummy_attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)
        
        print(f"üîÑ Converting to ONNX with input shape: {dummy_input_ids.shape}")
        
        # Export to ONNX
        torch.onnx.export(
            model,
            (dummy_input_ids, dummy_attention_mask),
            str(onnx_output_path),
            export_params=True,
            opset_version=14,  # Required for scaled_dot_product_attention
            do_constant_folding=True,
            input_names=['input_ids', 'attention_mask'],
            output_names=['last_hidden_state', 'pooler_output'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                'last_hidden_state': {0: 'batch_size', 1: 'sequence'},
                'pooler_output': {0: 'batch_size'}
            }
        )
        
        # Check output file size
        if onnx_output_path.exists():
            size_mb = onnx_output_path.stat().st_size / (1024 * 1024)
            print(f"‚úÖ ONNX conversion successful!")
            print(f"   - Output: {onnx_output_path}")
            print(f"   - Size: {size_mb:.1f} MB")
            
            # Verify the ONNX model can be loaded
            try:
                import onnx
                onnx_model = onnx.load(str(onnx_output_path))
                onnx.checker.check_model(onnx_model)
                print(f"‚úÖ ONNX model validation passed")
                return True
            except ImportError:
                print("‚ö†Ô∏è  ONNX package not available for validation, but conversion completed")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è  ONNX validation failed: {e}")
                return True  # Still consider success if file was created
        else:
            print("‚ùå ONNX file was not created")
            return False
            
    except Exception as e:
        print(f"‚ùå Conversion failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def install_requirements():
    """Install required packages if not available."""
    try:
        import transformers
        import torch
        print("‚úÖ Required packages already installed")
        return True
    except ImportError:
        print("üì¶ Installing required packages...")
        import subprocess
        import sys
        
        packages = [
            "torch",
            "transformers",
            "onnx"  # Optional for validation
        ]
        
        for package in packages:
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
                print(f"‚úÖ Installed {package}")
            except subprocess.CalledProcessError as e:
                print(f"‚ùå Failed to install {package}: {e}")
                if package != "onnx":  # ONNX is optional
                    return False
        
        return True

if __name__ == "__main__":
    print("üöÄ CodeBERT PyTorch ‚Üí ONNX Converter")
    print("=" * 40)
    
    # Install requirements
    if not install_requirements():
        print("‚ùå Failed to install required packages")
        exit(1)
    
    # Convert model
    if convert_codebert_to_onnx():
        print("\nüéâ Conversion completed successfully!")
        print("üí° The ONNX model is now ready for use with the Rust parser")
    else:
        print("\n‚ùå Conversion failed")
        exit(1)