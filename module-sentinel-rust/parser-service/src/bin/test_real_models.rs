use anyhow::Result;
use module_sentinel_parser::parsers::tree_sitter::ModelManager;
use std::path::Path;
use tracing::info;

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    tracing_subscriber::fmt::init();
    
    info!("ğŸš€ Testing real ML model integration");
    
    // Create models directory
    let models_dir = Path::new("models");
    std::fs::create_dir_all(models_dir)?;
    
    let model_manager = ModelManager::new(models_dir);
    
    // List available models
    let available = ModelManager::available_models();
    info!("ğŸ“‹ Available models:");
    for model in &available {
        info!("  â€¢ {} ({:.0}MB) - supports {:?}", 
              model.name, model.size_mb, model.languages);
    }
    
    #[cfg(feature = "ml")]
    {
        info!("ğŸ”„ ML feature enabled - attempting to download real models");
        
        // Test downloading the real model
        match model_manager.download_model("code_similarity").await {
            Ok(_) => {
                info!("âœ… Successfully downloaded CodeT5-small model");
                
                info!("âœ… Model infrastructure available for ML predictions");
            }
            Err(_) => {
                info!("âš ï¸ Model download failed - falling back to rule-based predictions");
            }
        }
    }
    
    #[cfg(not(feature = "ml"))]
    {
        info!("âš ï¸  ML feature disabled - using placeholder models");
        
        // Still test the infrastructure
        model_manager.download_model("code_similarity").await?;
        info!("âœ… Created placeholder model files");
        
        info!("âœ… Rule-based prediction infrastructure ready");
    }
    
    info!("ğŸ‰ Model integration test completed!");
    info!("ğŸ’¡ To enable real ML models, build with: cargo run --features ml --bin test_real_models");
    
    Ok(())
}