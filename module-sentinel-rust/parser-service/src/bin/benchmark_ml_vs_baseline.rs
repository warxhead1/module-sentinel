use anyhow::Result;
use module_sentinel_parser::services::{ParsingService, ParsingConfig};
use module_sentinel_parser::database::project_database::ProjectDatabase;
use std::path::Path;
use std::time::Instant;
use tracing::{info, warn};

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt::init();
    
    info!("ğŸ”¬ ML vs Baseline Benchmark Test");
    info!("=================================");
    
    let project_path = "/home/warxh/cpp_mcp_master/test-multi-proj/microservices-demo";
    let project_dir = Path::new(project_path);
    
    if !project_dir.exists() {
        warn!("âŒ Project directory not found: {}", project_path);
        return Ok(());
    }
    
    // Test 1: Baseline (no ML)
    info!("ğŸ“Š Test 1: Baseline Analysis (No ML)");
    let baseline_results = run_analysis_test("baseline", false).await?;
    
    // Test 2: With ML features
    info!("ğŸ“Š Test 2: ML-Enhanced Analysis");
    #[cfg(feature = "ml")]
    let ml_results = run_analysis_test("ml-enhanced", true).await?;
    
    #[cfg(not(feature = "ml"))]
    let ml_results = {
        warn!("âš ï¸  ML features not compiled - run with --features ml");
        // Return identical results for baseline comparison
        BenchmarkResult {
            total_files: baseline_results.total_files,
            total_symbols: baseline_results.total_symbols,
            total_relationships: baseline_results.total_relationships,
            parse_duration_ms: baseline_results.parse_duration_ms,
            error_count: baseline_results.error_count,
            success_rate: baseline_results.success_rate,
        }
    };
    
    // Compare results
    info!("ğŸ” Comparison Results:");
    info!("=====================");
    
    // Debug output
    info!("Baseline errors: {}, ML errors: {}", baseline_results.error_count, ml_results.error_count);
    
    compare_results("Baseline", &baseline_results, "ML-Enhanced", &ml_results);
    
    // ML Benefit Analysis
    let symbol_improvement = if baseline_results.total_symbols > 0 {
        ((ml_results.total_symbols as i64 - baseline_results.total_symbols as i64) as f64 / baseline_results.total_symbols as f64) * 100.0
    } else { 0.0 };
    
    let relationship_improvement = if baseline_results.total_relationships > 0 {
        ((ml_results.total_relationships as i64 - baseline_results.total_relationships as i64) as f64 / baseline_results.total_relationships as f64) * 100.0
    } else { 0.0 };
    
    let time_overhead = if baseline_results.parse_duration_ms > 0 {
        ((ml_results.parse_duration_ms as i64 - baseline_results.parse_duration_ms as i64) as f64 / baseline_results.parse_duration_ms as f64) * 100.0
    } else { 0.0 };
    
    info!("ğŸ“ˆ ML Performance Impact:");
    info!("  â€¢ Symbol detection: {:.1}% improvement", symbol_improvement);
    info!("  â€¢ Relationship detection: {:.1}% improvement", relationship_improvement);
    info!("  â€¢ Time overhead: {:.1}% slower", time_overhead);
    let error_diff = if baseline_results.error_count >= ml_results.error_count {
        format!("{} fewer errors", baseline_results.error_count - ml_results.error_count)
    } else {
        format!("{} more errors", ml_results.error_count - baseline_results.error_count)
    };
    info!("  â€¢ Error change: {}", error_diff);
    
    // Recommendation
    if symbol_improvement > 5.0 || relationship_improvement > 5.0 {
        info!("âœ… Recommendation: ML features provide measurable benefit");
    } else if time_overhead > 50.0 {
        warn!("âš ï¸  Recommendation: ML overhead too high for minimal benefit");
    } else {
        info!("â„¹ï¸  Recommendation: ML features ready but benefits are marginal");
    }
    
    #[cfg(feature = "ml")]
    {
        info!("ğŸ§ª ML Model Status:");
        info!("  â€¢ Real ONNX model: sentiment-analysis.onnx (255MB)");
        info!("  â€¢ Tokenization: Language-aware code tokenization active");
        info!("  â€¢ Fallback: Graceful degradation to rule-based analysis");
    }
    
    Ok(())
}

#[derive(Clone, Debug)]
struct BenchmarkResult {
    total_files: i32,
    total_symbols: i32,
    total_relationships: i32,
    parse_duration_ms: u64,
    error_count: usize,
    success_rate: f64,
}

async fn run_analysis_test(test_name: &str, enable_ml: bool) -> Result<BenchmarkResult> {
    let start_time = Instant::now();
    
    // Create temporary database for each test
    let temp_dir = std::env::temp_dir().join(format!("module-sentinel-{}", test_name));
    std::fs::create_dir_all(&temp_dir)?;
    
    let project_db = ProjectDatabase::new(&temp_dir).await?;
    
    let mut config = ParsingConfig::default();
    config.enable_semantic_analysis = enable_ml;
    
    let parsing_service = ParsingService::new(project_db, config).await?;
    
    info!("ğŸš€ Starting {} analysis...", test_name);
    
    let project_path = "/home/warxh/cpp_mcp_master/test-multi-proj/microservices-demo";
    let result = parsing_service.parse_project(
        Path::new(project_path), 
        &format!("microservices-demo-{}", test_name)
    ).await?;
    
    let duration = start_time.elapsed();
    
    let success_rate = if result.total_files > 0 {
        ((result.total_files - result.errors.len() as i32) as f64 / result.total_files as f64) * 100.0
    } else { 100.0 };
    
    info!("âœ… {} analysis completed in {:.2?}", test_name, duration);
    info!("  â€¢ Files: {}", result.total_files);
    info!("  â€¢ Symbols: {}", result.total_symbols);
    info!("  â€¢ Relationships: {}", result.total_relationships);
    info!("  â€¢ Errors: {}", result.errors.len());
    info!("  â€¢ Success rate: {:.1}%", success_rate);
    
    // Clean up
    let _ = std::fs::remove_dir_all(&temp_dir);
    
    Ok(BenchmarkResult {
        total_files: result.total_files,
        total_symbols: result.total_symbols,
        total_relationships: result.total_relationships,
        parse_duration_ms: duration.as_millis() as u64,
        error_count: result.errors.len(),
        success_rate,
    })
}

fn compare_results(name1: &str, result1: &BenchmarkResult, name2: &str, result2: &BenchmarkResult) {
    info!("ğŸ“Š {} vs {} Comparison:", name1, name2);
    info!("   Metric          {} | {}", 
          format!("{:>12}", name1), format!("{:>12}", name2));
    info!("   Files           {:>12} | {:>12}", result1.total_files, result2.total_files);
    info!("   Symbols         {:>12} | {:>12}", result1.total_symbols, result2.total_symbols);
    info!("   Relationships   {:>12} | {:>12}", result1.total_relationships, result2.total_relationships);
    info!("   Parse Time (ms) {:>12} | {:>12}", result1.parse_duration_ms, result2.parse_duration_ms);
    info!("   Errors          {:>12} | {:>12}", result1.error_count, result2.error_count);
    info!("   Success Rate    {:>11.1}% | {:>11.1}%", result1.success_rate, result2.success_rate);
}